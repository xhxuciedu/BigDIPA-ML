{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a pre-trained Inception V3 model for the task of automatic classification of breast cancer metastases in whole-slide images (WSI) of histological lymph node sections. This notebook is a simiplified version of the method proposed recently by Liu, Yun, et al. \"Detecting cancer metastases on gigapixel pathology images.\" arXiv preprint arXiv:1703.02442 (2017). https://arxiv.org/abs/1703.02442\n",
    "\n",
    "The image dataset was used as part of and can be obtained from Camelyon 16 (expired) or Camelyon 17 (on-going as of Sept 5, 2017) competition. Reference https://camelyon17.grand-challenge.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Software stacks required are:\n",
    "\n",
    "    Python 2.7\n",
    "    Keras 2.0.7\n",
    "    Tensorflow 1.3.0\n",
    "    Openslide 3.4.1\n",
    "    Openslide Python interface 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Make the Display width of each cell the width of the window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize a few normal and tumor patch examples below. The top row is examples of tumor patches. The bottom row is example of normal patches.\n",
    "\n",
    "*Note: We did not threshold any pixel values during the preprocessing/data curation step, therefore patches of empty spaces (all-white/grey as seen in the lower left corner subplot) pixels are present in our training/test set. One could conceivably threshold these patches out based on pixel values and simply predict them as non-tumor to save computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0) # enlarge images\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "img1 = np.load('/media/data/home/lizx/split/Tumor_Slide_070/Tumor/Tumor_Slide_070_Tumor_Patch_00100.npy')\n",
    "plt.title('Tumor patch from tumor slide')\n",
    "plt.imshow(img1)\n",
    "plt.subplot(2,2,2)\n",
    "img2 = np.load('/media/data/home/lizx/split/Tumor_Slide_044/Tumor/Tumor_Slide_044_Tumor_Patch_01038.npy')\n",
    "plt.title('Tumor patch from tumor slide')\n",
    "plt.imshow(img2)\n",
    "plt.subplot(2,2,3)\n",
    "img3 = np.load('/media/data/home/lizx/split/Normal_Slide_010/Normal/Normal_Slide_010_Normal_Patch_05000.npy')\n",
    "plt.title('Normal patch from Normal slide')\n",
    "plt.imshow(img3)\n",
    "plt.subplot(2,2,4)\n",
    "img3 = np.load('/media/data/home/lizx/split/Tumor_Slide_011/Normal/Tumor_Slide_011_Normal_Patch_02500.npy')\n",
    "plt.title('Normal patch from tumor slide')\n",
    "plt.imshow(img3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we load in the pre-trained 48-layers Inception V3 model using Keras API. The original paper introducing the Inception V3 can be found here: https://arxiv.org/abs/1512.00567\n",
    "The top layer was not included as we add global pooling layer and two fully-connected (dense) layers to modify the architecture to fit our particular dataset need.\n",
    "\n",
    "For a friendly tutorial on Inception modules and earlier version of Inception V1 (i.e. GoogLeNet), see https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/\n",
    "\n",
    "The ImageNet dataset used to pretrain the Inception V3 model is a 1M image dataset spanning 1000 object classes as part of Imagenet Large Scale Visual recognition Challenge 2012 (ILSVRC2012). You can reference more here: http://image-net.org/challenges/LSVRC/2012/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "# The tensorflow backend automatically pre-allocates all avaiable GPU resources/memory to your run. Therefore we\n",
    "# explicitly limit what GPU id is avialable to be used with the following command.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # set python to use only the GPU with id = 0. Can be set to \"0,2\" if want to use GPUs id 0 and 2.\n",
    "\n",
    "# Create the base pre-trained model. As we are dealing with a binary classification problem instead of 1000-class problem, \n",
    "# we do not include the top layer.\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "# add a global spatial average pooling layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a sigmoid (binary) layer\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a summary of our model and a few summary statistics. As the number of trainable parameters are large (tens of millions), this gives a good indication of how long training the entire architecture would take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since output shape is trimmed off due to character limits of the .summary() function, you can explictly print output shape layer by layer to see the dimensions that were cut off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit printout of layer's shape\n",
    "for layer in model.layers:\n",
    "    print(layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each WSI is too large (100,000x100,000 pixels at about ~ 2GB) to be loaded in memory, we need to write a generator to return image patches of size 299x299 pixel.\n",
    "\n",
    "299x299 is the standard input size required by Inception V3. It is also possible to change the input layer to accept an input image of a different size. \n",
    "\n",
    "Keras fit_generator() functions are not threadsafe, therefore we first create a theadsafe generator. See reference here: http://anandology.com/blog/using-iterators-and-generators/\n",
    "\n",
    "We use a 80/20 training/validation data split of the training data. As the dataset is heavily imbalanced with the vast majority of the training patches belonging to the NORMAL class, we\n",
    "sample in such a way to obtain a 50/50 TUMOR/NORMAL split on average per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "batch_size = 32 # batch size per gradient update (identical to what was used in Liu et al.)\n",
    "\n",
    "def preprocess_input(x):\n",
    "    \"\"\"Scales the image pixel values to be within the range of [-1,1]\n",
    "    for use with the Inception model.\n",
    "    \"\"\"\n",
    "    return 2*(x/255.0)-1.0\n",
    "\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            return self.it.next()\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def pathology_training_img_generator(dirpath):\n",
    "    \"\"\"A generator that samples from the training dataset and returns\n",
    "    a tensor of size (batch_size, img_width, img_heigth, number_of_channels).\n",
    "    \n",
    "    This generator also implements the sampling scheme to ensure class balance\n",
    "    \"\"\"\n",
    "    files = os.listdir(dirpath)\n",
    "    X = np.zeros((batch_size, 299, 299, 3)) # container for training image patches, with the number of rows equal to the batch size\n",
    "    Y = np.ones((batch_size)) # container for ground truth labels\n",
    "    \n",
    "    file_types = ['Tumor_Slide_', 'Normal_Slide_'] \n",
    "    # We use an 80/20 train/validation split\n",
    "    num_norm_slide = 128 # first 128 of 160 normal slides used for training\n",
    "    num_tumor_slide = 88 # first 88 of 110 tumor slide used for training\n",
    "    \n",
    "    # The following two variables denote the number of normal or tumor patches randomly sampled from each slide during the preprocessing step\n",
    "    # to create the training dataset. These values were appended to patch file names so we put a ceiling to the range of possible image \n",
    "    # patches during sampling in this function.\n",
    "    num_norm_patches_per_slide = 10400  \n",
    "    num_tumor_patches_per_slide = 1040 \n",
    "\n",
    "    while True:\n",
    "        for i in xrange(batch_size):\n",
    "            ftype = np.random.choice(file_types, p=[.75,.25]) # sample from Tumor Slide or Normal Slide with probability p and 1-p respectively\n",
    "            if ftype == 'Normal_Slide_':\n",
    "                file_num = \"%03d\" % np.random.randint(1,num_norm_slide + 1) # randomly sample 1 Tumor slide\n",
    "                num_patches = \"%05d\" % np.random.randint(0, num_norm_patches_per_slide, 1) # randomly sample a normal patch\n",
    "                patch_img = np.load(os.path.join(dirpath, ftype + file_num, 'Normal', \n",
    "                                        ftype + file_num + '_Normal_Patch_' + num_patches + '.npy'))\n",
    "                X[i,:,:,:] =  preprocess_input(patch_img)\n",
    "                Y[i] = 0\n",
    "            else:\n",
    "                file_num = \"%03d\" % np.random.randint(1,num_tumor_slide + 1)\n",
    "                num_patches = 0\n",
    "                patch_types = ['Tumor', 'Normal'] \n",
    "                ptype = np.random.choice(patch_types, p=[.67, .33]) # sample from Tumor and Normal patches with probability q and 1-q respectively\n",
    "                if ptype == 'Tumor':\n",
    "                    Y[i] = 1\n",
    "                    num_patches = \"%05d\" % np.random.randint(0, num_tumor_patches_per_slide, 1) # randomly sample a normal patch\n",
    "                else:\n",
    "                    Y[i] = 0\n",
    "                    num_patches = \"%05d\" % np.random.randint(0, num_norm_patches_per_slide,1) # randomly sample a tumor patch\n",
    "\n",
    "                patch_img = np.load(os.path.join(dirpath, ftype + file_num, ptype, \n",
    "                                    ftype + file_num + '_' + ptype + '_Patch_' + num_patches + '.npy'))\n",
    "                X[i,:,:,:] =  preprocess_input(patch_img)\n",
    "\n",
    "        yield(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we randomly sample patches from the remaining 20% split of the training data to used as validation set. We sample in such a way to maintain balance between Normal and Tumor patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_validation_data(dirpath):\n",
    "    \"\"\"The validation data is obtained from the remaining 20% slides. Due to the large computation time if all \n",
    "    validation data are used, only 1000 patches (500 normal and 500 tumor) are randomly selected from the\n",
    "    remaining slides.\n",
    "    \"\"\"\n",
    "    X_val_norm = np.zeros((500, 299, 299, 3))\n",
    "    Y_val_norm = np.zeros(500)\n",
    "    X_val_tumor = np.zeros((500, 299, 299, 3))\n",
    "    Y_val_tumor = np.ones(500)\n",
    "    \n",
    "    for i in xrange(500):\n",
    "        num_normal_slide = \"%03d\" % np.random.randint(128, 160, 1) # randomly sample a normal slide\n",
    "        num_normal_patch = \"%05d\" % np.random.randint(0, 10400, 1) # randomly sample a normal patch\n",
    "        ptype = 'Normal'\n",
    "        if num_normal_slide < 110:\n",
    "            ptype = np.random.choice(['Tumor', 'Normal'])\n",
    "        num_tumor_slide = \"%03d\" % np.random.randint(88, 110, 1) # randomly sample a tumor slide\n",
    "        num_tumor_patch = \"%05d\" % np.random.randint(0, 1040, 1) # randomly sample a tumor patch\n",
    "        X_val_norm[i,:,:,:] = preprocess_input(np.load(os.path.join(dirpath, ptype + '_Slide_' + num_normal_slide, 'Normal', \n",
    "                                ptype  + '_Slide_' + num_normal_slide + '_Normal_Patch_' + num_normal_patch + '.npy')))\n",
    " \n",
    "        X_val_tumor[i,:,:,:] = preprocess_input(np.load(os.path.join(dirpath, 'Tumor_Slide_' + num_tumor_slide, 'Tumor', \n",
    "                                    'Tumor_Slide_' + num_tumor_slide + '_Tumor_Patch_' + num_tumor_patch + '.npy')))\n",
    "    \n",
    "    # concatenate the two various design matrix and shuffle the rows\n",
    "    X_val = np.concatenate((X_val_norm, X_val_tumor), axis=0)\n",
    "    Y_val = np.concatenate((Y_val_norm, Y_val_tumor), axis=0)\n",
    "    X_val, Y_val = shuffle(X_val, Y_val, random_state=100)\n",
    "    return X_val, Y_val\n",
    "\n",
    "# generate validation data\n",
    "val_dirpath = '/media/data/home/lizx/split'\n",
    "X_val, Y_val = get_validation_data(val_dirpath)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note for convolutional models, lower layers represent more primitive shapes, such as lines (edges) and circles, and each successive layers represents higher, more abstract shapes until reaching the top layer which is the most directly related to the particular image at hand. See cell below for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure of visualization of CNN layers\n",
    "from IPython.display import Image\n",
    "Image(url='https://i.stack.imgur.com/Hl2H6.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, when finetuning a pretrained model, we would like to use a smaller learning rate at the lower layers (would like to maintain the general properties of these lower filters) and a high learning rate at the top layers to better fit our particular dataset. \n",
    "\n",
    "To achieve this mechanism in Keras, we use a similar 3-step training strategy as outlined by Shen, Li. \"End-to-end Training for Whole Image Breast Cancer Diagnosis using An All Convolutional Design.\" arXiv preprint arXiv:1708.09427 (2017).: https://arxiv.org/abs/1708.09427\n",
    "\n",
    "# 3-Step Training pipeline:\n",
    "    Step 1: set learning rate to 1e-3 and train the top 2 layers for 5 epochs.\n",
    "    Step 2: Set learning rate to 1e-4, unfreeze the top 2 inception module and train for 10 epochs,\n",
    "    Step 3: Set learning rate to 1e-5, unfreeze all layers and train for 50-100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Training: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=RMSprop(lr=1e-3, rho=.9, epsilon=1e-08, decay=0.0), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "\n",
    "# TensorBoard callback saves loss/accuracy data for visualization with tensorboard\n",
    "tf_board = TensorBoard(log_dir='tf_board_logs/step1', batch_size=batch_size, write_graph=True)\n",
    "\n",
    "# ModelCheckpoint callback allows user to save only the best performing model during the training phase\n",
    "filepath=\"checkpt_models/step1_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "model_checkpt = ModelCheckpoint(filepath, monitor='val_acc', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# CSVLogger callbacks writes loss/accuracy output per epoch to file for later post-processing and visualization\n",
    "csv_logger = CSVLogger('csv_logs/step1_train.log')\n",
    "\n",
    "callbacks_list = [tf_board, model_checkpt, csv_logger]\n",
    "\n",
    "history = model.fit_generator(pathology_training_img_generator('/media/data/home/lizx/split/'), \n",
    "                    epochs=5, steps_per_epoch=10, workers=1, callbacks=callbacks_list, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, we plot the Inception V3 architecture below. \n",
    "\n",
    "The last two inception modules are enclosed by \"Concat\" operations. This is equivalent to the named 'mixed' in the layer names output by Keras. Therefore if we scrolled up the previous cell, we see that layer 248 ('mixed8') is the last operation before the two last inception modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception V3 architecture\n",
    "Image(url='https://github.com/tensorflow/models/blob/master/inception/g3doc/inception_v3_architecture.png?raw=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 Training: we chose to train the top 2 inception modules, i.e. we will freeze\n",
    "# the first 248 layers and unfreeze the rest (this time fine-tuning the top 2 inception blocks\n",
    "# along with the top layers we added initially):\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# notice the smaller learning rate (lr)\n",
    "model.compile(optimizer=RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tf_board = TensorBoard(log_dir='tf_board_logs/step2', batch_size=batch_size, write_graph=True)\n",
    "filepath=\"checkpt_models/step2_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "model_checkpt = ModelCheckpoint(filepath, monitor='val_acc', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('csv_logs/step2_train.log')\n",
    "callbacks_list = [tf_board, model_checkpt, csv_logger]\n",
    "\n",
    "# For the sake of brevity, we set epochs to 10 and steps_per_epoch to 10.\n",
    "loss = model.fit_generator(pathology_training_img_generator('/media/data/home/lizx/split/'), \n",
    "                    epochs=10, steps_per_epoch=10, workers=1, callbacks=callbacks_list, \n",
    "                    validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Training: We unfreeze all layers and train the entire model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# notice the small learning rate (lr)\n",
    "model.compile(optimizer=RMSprop(lr=1e-5, rho=0.9, epsilon=1e-08, decay=0.0), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "tf_board = TensorBoard(log_dir='tf_board_logs/step3', batch_size=batch_size, write_graph=True)\n",
    "filepath=\"checkpt_models/step3_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "model_checkpt = ModelCheckpoint(filepath, monitor='val_acc', \n",
    "                                verbose=1, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger('csv_logs/step3_train.log')\n",
    "\n",
    "# For use when computational resources is limited. Can set criteria to terminate training.\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [tf_board, model_checkpt, csv_logger]\n",
    "\n",
    "# For the sake of brevity, we set epochs to 10 and steps_per_epoch to 10. These should be\n",
    "# longer, say epochs=50 and steps_per_epoch=100\n",
    "loss = model.fit_generator(pathology_training_img_generator('/media/data/home/lizx/split/'), \n",
    "                    epochs=10, steps_per_epoch=10, workers=1, callbacks=callbacks_list, \n",
    "                    validation_data=(X_val, Y_val))\n",
    "\n",
    "# Save learned model for prediction time\n",
    "#model.save('pathai_model.h5')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that the model has been fully trained, we are ready to make inference on new slides.\n",
    "\n",
    "# Test (Inference) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openslide\n",
    "from __future__ import division\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors as mcol\n",
    "import os\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "# set some memory parameters\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_inference_batch(batch_arr,model):\n",
    "    \"\"\"Use the provided model to make prediction on each batch array\n",
    "    \"\"\"\n",
    "    return model.predict(batch_arr)[:,0]\n",
    "\n",
    "def run_inference_across_slide(slide,model,patch_size=(299,299),stride=(128,128),start=(0,0),size='full',batch_size=1000):\n",
    "    \"\"\"Makes inference on a test slide by using a sliding window approach\n",
    "        patch_size: specifies size of patch images\n",
    "        stride: the stride size used by sliding window across the original pathology image.\n",
    "        start: starting pixel location for original pathology image to make inference on\n",
    "        size: defines the size (in pixel) of the original pathology image to make inference on relative to start location\n",
    "        batch_size: batch size per inference\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the slide using OpenSlide\n",
    "    if type(slide)==str:\n",
    "        slide_fh=openslide.OpenSlide(slide)\n",
    "    elif type(slide)==openslide.OpenSlide:\n",
    "        slide_fh=slide\n",
    "    else:\n",
    "        raise TypeError('Unknown slide type: ' + str(type(slide)))\n",
    "        \n",
    "    slide_size=slide_fh.level_dimensions[0] # We will make prediction at level 0, the full image scale\n",
    "    \n",
    "    # specifies start and end pixel location of window of interest in test slide\n",
    "    if size=='full':\n",
    "        end=slide_size\n",
    "        start=(0,0)\n",
    "    else:\n",
    "        end=start[0]+size[0],start[1]+size[1]\n",
    "    \n",
    "    # calculates size of heat map due to input values\n",
    "    heat_map_size=(end[0]-start[0]-patch_size[0])//stride[0]+1,(end[1]-start[1]-patch_size[1])//stride[1]+1\n",
    "    heat_map_list=[]\n",
    "    batch_list=[]\n",
    "    count=0\n",
    "    \n",
    "    # calculates total number of batches expected to perform inference on\n",
    "    total_batch_size = int(len(xrange(start[0],end[0]-patch_size[0]+1,stride[0]))*\n",
    "                                len(xrange(start[1],end[1]-patch_size[1]+1,stride[1]))/batch_size)\n",
    "    \n",
    "    # the actual window sliding inference part\n",
    "    for x in xrange(start[0],end[0]-patch_size[0]+1,stride[0]):\n",
    "        for y in xrange(start[1],end[1]-patch_size[1]+1,stride[1]):\n",
    "            # get image patch from OpenSlide\n",
    "            image=np.array(slide_fh.read_region((x,y),0,patch_size))[:,:,:3]\n",
    "            batch_list.append(image)\n",
    "            if len(batch_list)>=batch_size:\n",
    "                count+=1\n",
    "                print(str(count) + \" of \" + str(total_batch_size) + \" batches completed.\")\n",
    "                y_=run_inference_batch(np.stack(batch_list),model)\n",
    "                heat_map_list.append(y_)\n",
    "                batch_list=[]\n",
    "    if len(batch_list)!=0:\n",
    "        y_=run_inference_batch(np.stack(batch_list),model)\n",
    "        heat_map_list.append(y_)\n",
    "        batch_list=[]\n",
    "    heat_map=np.concatenate(heat_map_list).reshape(heat_map_size[1],heat_map_size[0],order='F').T\n",
    "    return heat_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set some directories\n",
    "NORMAL_SLIDES_DIR='/media/data/home/ysvang/Camelyon16/Train_Normal'\n",
    "TUMOR_SLIDES_DIR='/media/data/home/ysvang/Camelyon16/Train_Tumor'\n",
    "MASK_DIR='/media/data/home/ysvang/Camelyon16/Ground_Truth_Extracted/Mask'\n",
    "ANNO_DIR='/media/data/home/ysvang/Camelyon16/Ground_Truth_Extracted/XML'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back in the best model here\n",
    "model=load_model('checkpt_models/step3_weights-improvement-04-0.93.hdf5')\n",
    "\n",
    "# We will make prediction on a sample slide\n",
    "test_region_top_left=(68924,131761)\n",
    "slide_file='/media/data/home/ysvang/Camelyon16/Train_Tumor/Tumor_001.tif'\n",
    "heat=run_inference_across_slide(slide_file,model,start=test_region_top_left,size=(5000,5000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heat map for the slide\n",
    "plt.imshow(heat.T,cmap='jet',vmin=0,vmax=1)\n",
    "plt.gcf().set_size_inches((15,12))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This concludes this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
